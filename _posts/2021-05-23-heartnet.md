---
layout: post
title:  "AI in the Ambulance: Reducing Heart Attack Deaths by 50%"
author: sean
categories: [ Neural Networks, Machine Learning, AI, Healthcare Technology ]
image: assets/images/heartnet1/banner_image.jpg
featured: true
hidden: true
description: "Technical blog post about developing a convolutional neural network with FastAI to detect heart attack in 12-lead EKGs."
comments: false
---

*Bridging the gap between healthcare AI research and application by re-thinking neural network heart attack detection.*

*Special thanks to my collaborators Stephanie Yu, Lucky Lim, and Georgia Kenderova.*


## Project Background

Heart disease is the leading cause of death in America, beating out accidents, respiratory disease, and even cancer.  805,000 heart attacks occur in the US each year, and throughout my time in EMS, I'd usually respond to at least one 911 call for possible heart attack symptoms every day. [\[1\]](#Citations)

![Diagram illustrating a heart attack]({{site.baseurl}}/assets/images/heartnet1/heart_attack_illustration.jpg)

*Illustration of a heart attack*

A heart attack happens when plaque blocks the arteries that supply oxygen to the heart muscles, causing the muscles to die unless the oxygen supply can be quickly restored.  Surgical procedures can clear the blockage and thus save the heart muscles, but only if they're done quickly enough that the muscles haven’t already died.  The difference between life and death can be a matter of minutes, so quick diagnosis is critical.  The first test used to diagnose a heart attack is a 12-lead electrocardiogram (EKG): a procedure that makes a graph of the heart’s electrical activity, where trained medical providers can immediately see the signs of a heart attack.

![12-lead]({{site.baseurl}}/assets/images/heartnet1/12lead.jpg) 

*Demonstration of a 12-lead EKG: wires placed on the chest and limbs graph the heart's electrical activity over time.*

One of the biggest improvements in heart attack treatment has come from training EMS personnel to perform EKGs before patients get to the hospital.  One study found that this reduced death in patients with EKG-detectable heart attacks by almost **half**. [\[2\]](#Citations)  This is because if paramedics can detect a heart attack, they can rush the patient to treatment and radio ahead to the hospital so that the surgical team is assembled when they arrive.

<iframe width="560" height="315" src="https://www.youtube.com/embed/3CSdFBUMhpo?start=16&end=124" title="EKG during a heart attack" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

*Clip showing a typical EMS response to a heart attack.  Note how the paramedics performing an EKG allows them to detect the heart attack and quickly get the patient to treatment.*

Yet despite the proven lifesaving effect of pre-hospital EKGs, many EMS systems continue to rely on Basic Life Support (BLS) providers untrained in EKG interpretation.  This can happen due to funding and personnel shortages.  For example, the Detroit EMS system, which serves a population of ~670,000, is predominately staffed by BLS units, and Michigan has a statewide shortage of hundreds of EKG-trained paramedics. [\[3\]](#Citations)  The result is that in such systems, heart attacks go undetected before hospital arrival, resulting in slower diagnosis, slower treatment, and higher mortality.

As one of these BLS providers, I remember several calls where we missed a heart attack due to lack of EKG capability.  Nothing was more infuriating than waiting in traffic on the drive to the hospital, then waiting in the triage line for 10-20 minutes, only to finally have hospital staff do an EKG and find out that our patient had been having a heart attack the entire time.  The irreparable damage done to these patients' hearts could have been prevented if only we'd been able to do an EKG when we first got to them.  To my knowledge, Detroit—a city whose population is 78% Black—is the only place in Southeast Michigan that receives this lower quality of care.  Thus not only is such treatment an issue of preventable death, but of health equity and racial justice.

![parameds]({{site.baseurl}}/assets/images/heartnet1/paramedic.jpg) 

*Paramedics interpreting an EKG before arrival at the hospital.*

So how can we address this gap?  Performing an EKG is a relatively simple procedure of sticking leads to a patient's body, which can be done by anyone.  The part requiring currently-unavailable expertise is interpreting the results.  In my last months on the job, Detroit began trialing a system for BLS personnel to perform an EKG and wirelessly transmit it to the hospital for a doctor to review.  But frequently, poor network connectivity prevented the EKG from arriving before the ambulance did.

What if, instead of trying to send the results to a doctor somewhere else, a computer already onboard the ambulance could make the diagnosis?  Heart monitors already have an algorithm that tries to automatically interpret EKGs.  However, they currently suffer from poor reliability: one study found that a commonly used EMS monitor identified just 58% of STEMIs.  In other words, 42% of diagnosable heart attacks were missed by the monitor's algorithm—definitely not good enough when making life-and-death treatment decisions. [\[4\]](#Citations)

![lifepak12]({{site.baseurl}}/assets/images/heartnet1/Lifepak12.jpg)

*The Lifepak 12, the widely used EMS heart monitor in the aforementioned study.*

But over the past few years, researchers have dramatically improved these accuracy levels using machine learning.  Neural networks have learned to detect heart attack in EKGs with accuracy as high as 95% and have even outperformed trained healthcare providers. [\[6\], \[7\], \[8\]](#Citations) So why haven't we seen these methods widely applied to healthcare practice?  One reason is the fact that all such studies to my knowledge have been done on raw digital EKG signals, which in practice are typically plotted into a visual waveform before being displayed by the heart monitor.  Accessing the raw signals in the field would either need to be done on proprietary software run on EKG monitors themselves, or else on special hardware with the ability to interface directly with EKG monitors.  In either case, such implementation would be difficult both for researchers and healthcare providers.

Over the Spring 2021 semester, I explored an alternate approach that might resolve such difficulty.  EKG monitors are equipped with printing capabilities that are routinely used to record readings.  What if we could use neural networks to detect heart attack in pictures of these printouts?  If such a technique was developed, it could easily be built into a mobile phone application that lets providers photograph an EKG printout and instantly see the diagnosis.  Such an application could be built by researchers without needing special access to proprietary software on the heart monitor, and could then be deployed by healthcare agencies without needing to purchase any specialized equipment.  Thus the feasibility of an image-based classification approach for EKGs could represent a large step in bridging the gap between research achievements and healthcare practice.

![Ekg Strip]({{site.baseurl}}/assets/images/heartnet1/ekg_strip.jpg)

*An EKG strip printed out from a heart monitor: healthcare providers typically interpret these strips, rather than a live feed on the monitor.*

## Building our Dataset

The first challenge we faced was the lack of a publicly available dataset of annotated EKG printouts.  Neural networks typically need a large set of labeled examples, from which they learn to distinguish one category from another.  In our case, this would be a database of EKG images that have already been diagnosed by humans as "heart attack" or "normal".  But no such database exists online—every EKG dataset open to the public contains only digitized recordings of signals, which are a series of numbers recording voltage samples throughout the EKG.

So, to test the feasibility of our approach with publicly available data, we took one of these datasets and used it to simulate images similar to what a printout of each EKG would look like.  We worked with [PTB-XL](https://physionet.org/content/ptb-xl/1.0.1/), a collection of 21,837 EKGs published by [Physionet](https://physionet.org/).  We first simply used MatPlotLib to make a graph of the numbers in each recording, then OpenCV to concatenate the graph for each 12 leads in an EKG together into a single image.  The results weren't pretty:

![Image from Dataset 1]({{site.baseurl}}/assets/images/heartnet1/dataset_1.png)

*Our first attempt at generating images.  Looking rough.*

![Real EKG for comparison]({{site.baseurl}}/assets/images/heartnet1/ekg.jpg)

*An image of real EKG printout for comparison*

This approach had a number of issues.  First, repeated large vertical spikes appear in the images that aren't present on real EKGs.  These represent individual high or low outlier points in the recording.  Every EKG contains some amount of noise picked up from the environment and patient movement during the procedure.  Real heart monitors filter out some of this noise out for readability, but no such filter is applied in our images.  Another issue is that our waveform is much choppier-looking than real EKGs; this is partially due to the fact that our dataset recordings are sampled at 100Hz, while typical EKG printouts are in the 40-60Hz range, resulting in more fine-grain fluctuations being captured on our graphs.  It also seems likely that some additional "smoothing" is applied by the monitor for readability.

Interestingly, however, we were able to train a convolutional neural network to 89% accuracy on these images.  So while they don't look much like the images we were hoping to simulate, they do sucessfully visualize the important diagnostic features in each record and offer preliminary evidence that image-based EKG classification is possible.

To improve these issues, we went back to the drawing board.  This time, instead of directly inputting the data to MatPlotLib, we used the [WFDB-Python](https://wfdb.readthedocs.io/en/latest/index.html) library, which eliminates both the spikes and the choppiness in the resulting graph.  We then modified the source code to standardize the y-axis of each lead and add a background grid similar to real EKG printouts.  This resulted in the following images:

![Image from Dataset 2]({{site.baseurl}}/assets/images/heartnet1/dataset_2.png)

*Our second, improved attempt at generating images.*

Looking much better!  Other than every lead being stacked vertically, the images look fairly realistic.  But during field use, it's unlikely that it will ever deal with images quite this clean.  Pictures taken with a mobile camera are usually slanted, rotated, and of course have shadows fall on them:

![shadow image]({{site.baseurl}}/assets/images/heartnet1/shadow1.jpg)

![shadow image2]({{site.baseurl}}/assets/images/heartnet1/shadow2.jpg)

*Shadows appearing on real photos of EKG printouts taken with a mobile phone.*

To see if our model can handle these sorts of artifact, we simulated shadows on our images using a [library designed for road image augmentation](https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library):

![Image from Dataset 3]({{site.baseurl}}/assets/images/heartnet1/dataset_3.png)

*Our generated images with fake "shadows" added.*

While their shapes look laughably fake, these "shadows" nevertheless darken parts of the picture in a way that's very similar to real shadows.  So despite appearing unrealistic to the human eye, they do provide a good test for whether our model could handle the kind of distortions made by shadows on real-world images.  We also used FastAI's [data augmentation features](https://docs.fast.ai/vision.augment.html#aug_transforms) to apply rotation, warp, and brightness transformations during training.

## Developing the Model

After building our dataset, it was time to start training!  [Convolutional Neural Networks (CNNs)](https://cs231n.github.io/convolutional-networks/) are often well-suited to image recognition and have been used for most previous EKG-classification research.  To quickly develop an accurate model, we used [FastAI's `cnn_learner`](https://docs.fast.ai/vision.learner.html#cnn_learner), which employs transfer learning from [ResNet](https://arxiv.org/pdf/1512.03385.pdf) models—a set of award-winning neural networks pre-trained for general image recognition.

We started development with hyperparameter tuning: testing different ResNet architectures, batch sizes, and number of epochs on a subset of our data.  We then used the best-performing combination of hyperparameters for training on our full datasets.  With FastAI's library, this training was able to be accomplished in just 15 lines of code!

```python
import fastbook
fastbook.setup_book()
from fastbook import *
from fastai.vision.widgets import *

# Pick a GPU with free resources
torch.cuda.set_device(2)

# Get images
image_path = Path('/path/to/dataset')
images = get_image_files(image_path)

# Initialize metric functions
recall_function = Recall(pos_label=0)
precision_function = Precision(pos_label=0)
f1_score = F1Score(pos_label=0)

# Initialize DataLoader
images_datablock = DataBlock(
    blocks=(ImageBlock, CategoryBlock), 
    get_items=get_image_files, 
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    batch_tfms=aug_transforms(do_flip=False)
)
dls = images_datablock.dataloaders(image_path, bs=16)

# Create, train, and save model
learn = cnn_learner(dls, resnet152, metrics=[error_rate, recall_function, precision_function, f1_score])
learn.fine_tune(16)
learn.export('trained_model.pkl')
```

*The full code for training, short and sweet thanks to FastAI*

## Results

We made our final trainings with ResNet 152 and batch size of 16 for 25 epochs, the combination yielding the best results in our hyperparameter tuning.  These were our results:

![A picture of our final results]({{site.baseurl}}/assets/images/heartnet1/dataset_comparison.png)

*Comparison between results for clean images and shadows added*

We used four [metrics](https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/#:~:text=Recall%20is%20a%20metric%20that,indication%20of%20missed%20positive%20predictions) to measure our model's performance:
- **Accuracy** represents our model's overall rate of error. It measures how often the prediction is wrong in any way (both mistaking a heart attack as a normal EKG and vice versa).
- **Recall** (also known as sensitivity) specifically measures how often the model misses a heart attack. Recall is especially important because our dataset has more normal EKGs (66%) than heart attacks (34%).  So consider what might happen, for example, if our model predicted that every EKG was normal.  It would have an overall accuracy of 66%, which doesn't sound terrible.  But it would also be catching 0% of heart attacks—not what we want to see!
- **Precision** (also known as specificity) measures how many false positives our model produces: that is, when it mislabels a normal EKG as a heart attack.  This is important because a false positive could potentially lead to invasive surgical procedures being performed on someone who doesn't need them.  Precision is also an important metric to contextualize recall; for example, a model could achieve 100% recall by labeling every EKG as a heart attack.  But this obviously wouldn't be very useful in practice.
- **F1 Score** provides a single measure of both recall and precision, calculated via this formula: `(2*recall*precision) / (recall*precision)`.

Excitingly, our model has an **overall accuracy of 90%**!  Also excitingly, shadows on our images didn't substantially reduce a CNN's accuracy, indicating its potential to perform despite the distortions produced in real-world conditions. Less excitingly, our model's recall score is substantially lower at 70.2%, meaning it's **missing 29.8% of heart attacks**.  This is too many when every missed heart attack could mean someone dying.  However, it's better than both general physicians and Emergency Room Residents, who have miss 34.1% and 36.8% of heart attacks respectively. [\[6\]](#Citations)  It's also a substantial improvement over the algorithm used in the LifePak 12, which is even worse at 42%. [\[4\]](#Citations)

Overall, our results suggest the feasibility of image-based EKG classification, although they also point to the need to augment transfer learning with problem-specific techniques to achieve better results. ER physicians have a overall error rate of 3% [\[5\]](#Citations), and prior non-image based neural networks have achieved recall scores as high as 95%. [\[7\]](#Citations)  Techniques from this prior work could potentially be used to improve our own results, such as lead pooling and sub 2D convolutional layers.  Pre-processing images for noise reduction and pulse segmentation are two other techniques that might be applied from prior work. [\[8\]](#Citations)

To show how our model might be used by healthcare providers, we also built a simple [web application](https://github.com/oapostrophe/HeartNet/blob/main/app.py) using [StreamLit](https://streamlit.io/), where the user can upload an image and see the classification as well as the certainty of the prediction.

![A picture of the web application we built for our app]({{site.baseurl}}/assets/images/heartnet1/streamlit_demo.png)

*Our results put into a web application: upload an EKG image and get an immediate diagnosis.*

If you're interested in reading more, our full [write-up](https://oapostrophe.github.io/HeartNet/) and [source code](https://github.com/oapostrophe/HeartNet) are available on GitHub.  Feel free to reach out with any questions or comments, and thanks for reading!


<h2><a id="Citations">Citations</a></h2>

1. [Heart disease and stroke statistics—2021 update](https://www.heart.org/-/media/phd-files-2/science-news/2/2021-heart-and-stroke-stat-update/2021_heart_disease_and_stroke_statistics_update_fact_sheet_at_a_glance.pdf?la=en): a report from the American Heart Association
2. Bång, Angela, Lars Grip, Johan Herlitz, Stefan Kihlgren, Thomas Karlsson, Kenneth Caidahl, and Marianne Hartford. ["Lower mortality after prehospital recognition and treatment followed by fast tracking to coronary care compared with admittance via emergency department in patients with ST-elevation myocardial infarction."](https://www.sciencedirect.com/science/article/pii/S0167527307016579?casa_token=QwM9I3I5klIAAAAA:1BTMwOBPmN4yl27K4MK_dxenVVpPWVXrzWEmp2Sid99Vjj-018TLvvhR7CRVz5MGYgCmvs4a_A) International journal of cardiology 129, no. 3 (2008): 325-332.
3. Turner, A., Dunne, R. and Wise, K., 2017. National Institute For Health Care Reform. [online] Nihcr.org. Available at: [<https://nihcr.org/wp-content/uploads/2017/06/NIHCR_Altarum_Detroit_EMS_Brief_5-30-17.pdf>](https://nihcr.org/wp-content/uploads/2017/06/NIHCR_Altarum_Detroit_EMS_Brief_5-30-17.pdf) [Accessed 8 May 2021].
4. Mary Colleen Bhalla, Francis Mencl, Mikki Amber Gist, Scott Wilber & Jon Zalewski (2013) [Prehospital Electrocardiographic Computer Identification of ST-segment Elevation Myocardial Infarction](https://www.tandfonline.com/doi/abs/10.3109/10903127.2012.722176), Prehospital Emergency Care, 17:2, 211-216, DOI: 10.3109/10903127.2012.722176
5. Hartman, Stephanie M., Andrew J. Barros, and William J. Brady. ["The use of a 4-step algorithm in the electrocardiographic diagnosis of ST-segment elevation myocardial infarction by novice interpreters."](https://emupdates.com/wp-content/uploads/2008/07/Hartman-4-Steps-to-STEMI-Diagnosis-AmJEM-2012.pdf) The American journal of emergency medicine 30, no. 7 (2012): 1282-1295.
6. Mehta, S., F. Fernandez, C. Villagran, A. Frauenfelder, C. Matheus, D. Vieira, M. A. Torres et al. ["P1466 Can physicians trust a machine learning algorithm to diagnose ST elevation myocardial infarction?."](https://academic.oup.com/eurheartj/article-abstract/40/Supplement_1/ehz748.0231/5598215) European Heart Journal 40, no. Supplement_1 (2019): ehz748-0231.
7. Liu, Wenhan, Mengxin Zhang, Yidan Zhang, Yuan Liao, Qijun Huang, Sheng Chang, Hao Wang, and Jin He. ["Real-time multilead convolutional neural network for myocardial infarction detection."](https://ieeexplore.ieee.org/document/8103330) IEEE journal of biomedical and health informatics 22, no. 5 (2017): 1434-1444.
8. Park, Yeonghyeon, Il Dong Yun, and Si-Hyuck Kang. ["Preprocessing method for performance enhancement in cnn-based stemi detection from 12-lead ecg."](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771175) IEEE Access 7 (2019): 99964-99977.
9. Xiao, Ran, Yuan Xu, Michele M. Pelter, David W. Mortara, and Xiao Hu. ["A deep learning approach to examine ischemic ST changes in ambulatory ECG recordings."](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5961830/) AMIA Summits on Translational Science Proceedings 2018 (2018): 256.